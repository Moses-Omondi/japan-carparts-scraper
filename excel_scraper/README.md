# ğŸš€ Excel Scraper

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

A high-performance, async web scraper optimized for extracting e-commerce product data and exporting to Excel format.

## âœ¨ Features

- **ğŸš€ Blazing Fast**: Async/concurrent processing with up to 8 simultaneous requests
- **ğŸ“Š Excel Export**: Professional Excel files with formatting and multiple sheets
- **ğŸ’° Multi-Currency**: Automatic price detection for KES, USD, JPY, EUR, GBP
- **ğŸ¯ Smart Extraction**: Intelligent product data extraction from various e-commerce sites
- **ğŸ”§ Configurable**: Extensive configuration options for different use cases
- **ğŸ›¡ï¸ Respectful**: Built-in rate limiting and error handling
- **ğŸ“ˆ Production Ready**: Professional code structure with proper logging and error handling

## ğŸƒâ€â™‚ï¸ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/Moses-Omondi/japan-carparts-scraper.git
cd japan-carparts-scraper/excel_scraper

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install the package in development mode
pip install -e .
```

### Basic Usage

```bash
# Scrape Polish Venture shop
python scripts/scrape.py https://polishventure.com/shop/

# Scrape with custom parameters
python scripts/scrape.py https://example.com/products \
    --max-products 200 \
    --concurrent-requests 10 \
    --output-dir results
```

### Python API

```python
import asyncio
from excel_scraper import FastExcelScraper, Config

async def main():
    # Create configuration
    config = Config()
    config.set('max_concurrent_requests', 8)
    config.set('max_pages', 5)
    
    # Initialize scraper
    scraper = FastExcelScraper(config)
    
    try:
        # Scrape website
        products = await scraper.scrape_website_async('https://polishventure.com/shop/')
        
        # Save to Excel
        output_file = scraper.save_to_excel(products)
        print(f"Scraped {len(products)} products to {output_file}")
        
    finally:
        await scraper.close_session()

# Run the scraper
asyncio.run(main())
```

## ğŸ“‹ Command Line Options

| Option | Description | Default |
|--------|-------------|---------|
| `url` | Website URL to scrape | Required |
| `--max-products` | Maximum products to scrape | 100 |
| `--max-pages` | Maximum pages to scrape | Auto-calculated |
| `--concurrent-requests` | Simultaneous requests | 8 |
| `--delay` | Delay between requests (seconds) | 0.3 |
| `--timeout` | Request timeout (seconds) | 20 |
| `--output-dir` | Output directory | `output` |
| `--config` | Custom config file | None |
| `--verbose` | Enable verbose logging | False |

## ğŸ¯ Supported Sites

Currently optimized for:
- **Polish Venture** - Full support with specialized selectors
- **Generic E-commerce** - Works with most standard e-commerce sites
- **Custom Sites** - Easy to extend for new sites

## ğŸ“Š Excel Output

Generated Excel files include:

### Product Data Columns
- Product ID, Name, Description
- Prices in multiple currencies (KES, USD, JPY, EUR, GBP)
- SKU, Product URL, Source URL
- Scraped timestamp

### Formatting Features
- Professional headers with colors
- Auto-sized columns
- Data validation
- Clean, readable layout

## âš™ï¸ Configuration

### Default Configuration

```python
{
    "max_concurrent_requests": 8,
    "request_delay": 0.3,
    "timeout": 20,
    "max_products_per_page": 50,
    "max_pages": 20,
    "output_dir": "output",
    "file_prefix": "scraped_products"
}
```

### Custom Configuration File

```json
{
    "max_concurrent_requests": 10,
    "request_delay": 0.2,
    "timeout": 30,
    "polish_venture": {
        "base_url": "https://polishventure.com/shop/",
        "product_selectors": [
            ".product-item",
            ".product",
            ".woocommerce-loop-product__link"
        ]
    }
}
```

## ğŸ”§ Development

### Project Structure

```
excel_scraper/
â”œâ”€â”€ src/excel_scraper/          # Main package
â”‚   â”œâ”€â”€ __init__.py            # Package initialization
â”‚   â”œâ”€â”€ scraper.py             # Core scraper class
â”‚   â”œâ”€â”€ config.py              # Configuration management
â”‚   â””â”€â”€ utils.py               # Utility functions
â”œâ”€â”€ scripts/                    # Command line scripts
â”‚   â””â”€â”€ scrape.py              # Main CLI script
â”œâ”€â”€ tests/                      # Test suite
â”œâ”€â”€ docs/                       # Documentation
â”œâ”€â”€ config/                     # Configuration files
â”œâ”€â”€ examples/                   # Usage examples
â””â”€â”€ output/                     # Default output directory
```

### Running Tests

```bash
# Install development dependencies
pip install -e ".[dev]"

# Run tests
pytest tests/

# Run tests with coverage
pytest --cov=excel_scraper tests/
```

### Code Quality

```bash
# Format code
black src/ scripts/ tests/

# Sort imports
isort src/ scripts/ tests/

# Lint code
flake8 src/ scripts/ tests/

# Type checking
mypy src/
```

## ğŸš¨ Performance

### Benchmarks
- **~25 products/second** with default settings
- **10x faster** than synchronous alternatives
- **Memory efficient** with streaming processing
- **Respectful** with configurable rate limiting

### Optimization Tips
1. Increase `max_concurrent_requests` for faster sites
2. Decrease `request_delay` if site allows
3. Use `max_pages` to limit scope
4. Enable verbose logging only for debugging

## ğŸ›¡ï¸ Best Practices

### Respectful Scraping
- Built-in delays between requests
- Configurable request limits
- Proper error handling and retries
- Respectful User-Agent headers

### Error Handling
- Graceful handling of network errors
- Automatic retry with backoff
- Comprehensive logging
- Clean session management

## ğŸ“ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ¤ Support

- **Issues**: [GitHub Issues](https://github.com/Moses-Omondi/japan-carparts-scraper/issues)
- **Discussions**: [GitHub Discussions](https://github.com/Moses-Omondi/japan-carparts-scraper/discussions)
- **Email**: your.email@example.com

## ğŸ™ Acknowledgments

- Built with [aiohttp](https://aiohttp.readthedocs.io/) for async HTTP
- Uses [pandas](https://pandas.pydata.org/) for data processing
- Excel export with [openpyxl](https://openpyxl.readthedocs.io/)
- HTML parsing with [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/)

---

âš¡ **Built for speed, designed for professionals** âš¡